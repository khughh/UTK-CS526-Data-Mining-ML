{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "khVZ7DoGDiBn",
   "metadata": {
    "id": "khVZ7DoGDiBn"
   },
   "source": [
    "# **`M.1.Introduction to data mining`**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f09fcd7",
   "metadata": {
    "id": "5f09fcd7"
   },
   "source": [
    "## **`assign.M.1.assignment.1`** - Data Mining with Covid Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ujMShg_DwKdx",
   "metadata": {
    "id": "ujMShg_DwKdx"
   },
   "source": [
    "### **`Overview and Directions`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orcAiqUZAQ6Y",
   "metadata": {
    "id": "orcAiqUZAQ6Y"
   },
   "source": [
    "1. Import and manipulate a .csv file  \n",
    "2. Assess your Python Programming Skills  \n",
    "3. Other assignments are more challenging; use this to assess your skills.\n",
    "4. Prepare questions for a class discussion to help source additional tools. \n",
    "5. Perform tasks without assistance from clever sources.    \n",
    "\n",
    "#### **`Desired outcomes`**  \n",
    "- Experience Pandas dataframes to group, aggregate, find, sort, and calculate.  \n",
    "- Perform calculations to find best country, rank, and total items processed. \n",
    "- Note: Pandas is reviewed in Module 2 and quality resources provided below.  \n",
    "\n",
    "#### **`Additional resources`**  \n",
    "- [Daniel Chen](https://github.com/chendaniely/) is a **generous** Pandas master.  \n",
    "=> Purchase of his books is recommended; not a solicitation!    \n",
    "- [Chen,D.,(2022). Pandas for everyone, 2nd.Ed.](https://www.amazon.com/Pandas-Everyone-Analysis-Addison-Wesley-Analytics/dp/0137891156/ref=sr_1_1?crid=T9BF3HU24YFL&keywords=pandas+for+everyone&qid=1685205022&sprefix=pandas+for+everyone%2Caps%2C203&sr=8-1)  \n",
    "=> [groupby](https://github.com/chendaniely/2017-10-26-python_crash_course/blob/gh-pages/notebooks/07-groupby.ipynb) => [missing values](https://github.com/chendaniely/2017-10-26-python_crash_course/blob/gh-pages/notebooks/03-missing.ipynb) => [many more!](https://github.com/chendaniely/2017-10-26-python_crash_course/tree/gh-pages/notebooks)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RNiLo5Rm9SM2",
   "metadata": {
    "id": "RNiLo5Rm9SM2"
   },
   "source": [
    "### **`Task.0`**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wcrKuRsWXUW1",
   "metadata": {
    "id": "wcrKuRsWXUW1"
   },
   "source": [
    "#### **`Dataset`**\n",
    "- source information => COVID-19 variant [sequencing](https://www.cdc.gov/coronavirus/2019-ncov/variants/genomic-surveillance.html#:~:text=Scientists%20use%20a%20process%20called%20genomic%20sequencing%20to%20identify%20SARS,test%20positive%20for%20COVID%2D19) by countries.   \n",
    "Data fields    \n",
    "1. `location`: the country providing information.    \n",
    "2. `date`: data entry date.  \n",
    "3. `variant`: the COVID-19 variant for the entered record.  \n",
    "4. `num_sequences`: the number of sequences **processed** by country, variant, and date.   \n",
    "5. `num_sequences_total`: the number of sequences **available** by country, variant, and date.  \n",
    "6. `perc_sequences`: the percentage of the available sequences processed (*out of 100*)  \n",
    "`note:` each dataset row represents *one* variant by *one* country on *one* day.  \n",
    "\n",
    "**`Tasks`**  \n",
    "1. Locate and read dataset into a pandas.DataFrame called 'df' via  \n",
    "a. A Kaggle API; use existing or acquire; [Kaggle.covid.dataset](https://www.kaggle.com/yamqwe/omicron-covid19-variant-daily-cases?select=covid-variants.csv)  \n",
    "or  \n",
    "b. Class github URL or another .csv method like [Matthes, Ch.16](https://github.com/cosc-526/cosc.526.home.page/blob/main/textbook.Python.crash.course.matthes.pdf)    \n",
    "=> filename: [data.M.1.assignment.covid.data.csv](https://raw.githubusercontent.com/cosc-526/home.page/main/data.M.1.assignment.covid.data.csv)    \n",
    "=>**consider** reading a Github data URL requires a path to **raw data**    \n",
    "2. Display the DataFrame's first 5 rows.  \n",
    "3. Display descriptive stats confirming: 100,416 data records.  \n",
    "4. Round DataFrame to 1 decimal place!   \n",
    "\n",
    "**`Useful links`**  \n",
    "[Built-in Functions](https://docs.python.org/3/library/functions.html#built-in-functions)  \n",
    "[pandas.DataFrame documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "J6JeWBO2Cw78",
   "metadata": {
    "id": "J6JeWBO2Cw78",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Header \n",
      "   location        date    variant  num_sequences  perc_sequences  \\\n",
      "0   Angola  2020-07-06      Alpha              0             0.0   \n",
      "1   Angola  2020-07-06  B.1.1.277              0             0.0   \n",
      "2   Angola  2020-07-06  B.1.1.302              0             0.0   \n",
      "3   Angola  2020-07-06  B.1.1.519              0             0.0   \n",
      "4   Angola  2020-07-06    B.1.160              0             0.0   \n",
      "\n",
      "   num_sequences_total  \n",
      "0                    3  \n",
      "1                    3  \n",
      "2                    3  \n",
      "3                    3  \n",
      "4                    3  \n",
      "Dataframe descriptive statistics, rounded to tenths \n",
      "        num_sequences  perc_sequences  num_sequences_total\n",
      "count       100416.0        100416.0             100416.0\n",
      "mean            72.2             6.2               1509.6\n",
      "std           1669.3            21.9               8445.3\n",
      "min              0.0            -0.0                  1.0\n",
      "25%              0.0             0.0                 12.0\n",
      "50%              0.0             0.0                 59.0\n",
      "75%              0.0             0.0                394.0\n",
      "max         142280.0           100.0             146170.0\n",
      "Number of Records: 100416\n"
     ]
    }
   ],
   "source": [
    "#=>Enter answer  \n",
    "import pandas as pd\n",
    "df = pd.read_csv('/Users/kevhhu/Desktop/UTK CS526/covid-variants.csv')\n",
    "#DF Header \n",
    "print('DataFrame Header \\n', df.head())\n",
    "\n",
    "#Rounding stats\n",
    "stats = df.describe()\n",
    "stats_r = stats.round(1) #Assuming you want rounded to 1 decimal, else stats.round(0) would round to whole numbers and match the expected outcome \n",
    "print('Dataframe descriptive statistics, rounded to tenths \\n', stats_r)\n",
    "\n",
    "print('Number of Records:',df.shape[0])    # Should print 100416\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4z6COkzkC2KU",
   "metadata": {
    "id": "4z6COkzkC2KU"
   },
   "source": [
    "#### **`Task.0 - Expected Outcome`**  \n",
    "```\n",
    "DataFrame header\n",
    "  location  date        variant  num_sequences  perc_sequences  num_sequences_total\n",
    "0   Angola  2020-07-06      Alpha              0             0.0   3\n",
    "1   Angola  2020-07-06  B.1.1.277              0             0.0   3\n",
    "2   Angola  2020-07-06  B.1.1.302              0             0.0   3\n",
    "3   Angola  2020-07-06  B.1.1.519              0             0.0   3\n",
    "4   Angola  2020-07-06    B.1.160              0             0.0   3\n",
    "\n",
    "Dataframe descriptive statistics, rounded to tenths\n",
    "       num_sequences  perc_sequences  num_sequences_total\n",
    "count       100416.0        100416.0             100416.0\n",
    "mean            72.0             6.0               1510.0\n",
    "std           1669.0            22.0               8445.0\n",
    "min              0.0            -0.0                  1.0\n",
    "25%              0.0             0.0                 12.0\n",
    "50%              0.0             0.0                 59.0\n",
    "75%              0.0             0.0                394.0\n",
    "max         142280.0           100.0             146170.0 \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760c5a8f",
   "metadata": {
    "id": "760c5a8f"
   },
   "source": [
    "### **`Task.1`** - Find uncommon variants  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZGy_VBO4DDyd",
   "metadata": {
    "id": "ZGy_VBO4DDyd"
   },
   "source": [
    "The U.S. experienced the COVID-19 `Alpha`, `Delta`, `Omicron`  \n",
    "\n",
    "**`Tasks`**  \n",
    "0. In whatever object you like, e.g. list, dataframe, etc  \n",
    "1. Get unique variant items for category: **`US_and_other`**  \n",
    "=> where variants == [US, `non_who`, `others`]  \n",
    "2. Get unique variant items for category: **`nonUS_and_other`**  \n",
    "=> where variants != [US, `non_who`, `others`]  \n",
    "3. Print your chosen objects to display unique variant categories.  \n",
    "4. Show a total unique count for each, and total for dataset,\n",
    "\n",
    "**`Useful links`**   \n",
    "- [len()](https://docs.python.org/3/library/functions.html#len)\n",
    "- [list comprehension w Bro Code](https://www.youtube.com/watch?v=fcLDzKH_5XM)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "vxMduRg5Cj5u",
   "metadata": {
    "id": "vxMduRg5Cj5u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US_and_other unique variants:\n",
      " {'Omicron', 'others', 'Delta', 'non_who', 'Alpha'}\n",
      "nonUS_and_other unique variants:\n",
      " {'B.1.367', 'Iota', 'B.1.1.277', 'B.1.258', 'B.1.1.302', 'S:677P.Pelican', 'Beta', 'B.1.620', 'Lambda', 'Mu', 'B.1.177', 'Kappa', 'B.1.221', 'B.1.160', 'Epsilon', 'Gamma', 'Eta', 'B.1.1.519', 'S:677H.Robin1'}\n",
      "Total unique variants in dataset:\n",
      " {'B.1.367', 'Iota', 'B.1.1.277', 'Delta', 'non_who', 'B.1.258', 'B.1.1.302', 'Omicron', 'S:677P.Pelican', 'Beta', 'B.1.620', 'Lambda', 'Mu', 'B.1.177', 'Kappa', 'B.1.221', 'B.1.160', 'others', 'Epsilon', 'Gamma', 'Eta', 'B.1.1.519', 'Alpha', 'S:677H.Robin1'}\n",
      "\n",
      "US_and_other unique count: 5\n",
      "nonUS_and_other unique count: 19\n",
      "Total unique variant count: 24\n"
     ]
    }
   ],
   "source": [
    "#=>Enter answer \n",
    "#Did not do this right initially, setting list with known variant names as stated.\n",
    "us_and_other = [\n",
    "    'Alpha', 'Delta', 'Omicron', 'others', 'non_who'\n",
    "]\n",
    "\n",
    "us_and_other_variants = set(df[df['variant'].isin(us_and_other)]['variant'])\n",
    "\n",
    "nonus_and_other_variants = set(df[~df['variant'].isin(us_and_other)]['variant']) #difference \n",
    "\n",
    "total_unique_variants = set(df['variant'])\n",
    "\n",
    "print(\"US_and_other unique variants:\\n\", us_and_other_variants)\n",
    "print(\"nonUS_and_other unique variants:\\n\", nonus_and_other_variants)\n",
    "print(\"Total unique variants in dataset:\\n\", total_unique_variants)\n",
    "\n",
    "print(\"\\nUS_and_other unique count:\", len(us_and_other_variants))\n",
    "print(\"nonUS_and_other unique count:\", len(nonus_and_other_variants))\n",
    "print(\"Total unique variant count:\", len(total_unique_variants))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jBOVMJ0zck9a",
   "metadata": {
    "id": "jBOVMJ0zck9a"
   },
   "source": [
    "#### **`Task.1 - Expected Outcome`**  \n",
    "```\n",
    "note: organization of output can vary widely!  \n",
    "\n",
    "['Alpha', 'Delta', 'Omicron', 'others', 'non_who']  \n",
    "\n",
    "total US + other =  5\n",
    "\n",
    "['B.1.1.277', 'B.1.1.302', 'B.1.1.519', 'B.1.160', 'B.1.177', 'B.1.221',  \n",
    " 'B.1.258', 'B.1.367', 'B.1.620', 'Beta', 'Epsilon', 'Eta', 'Gamma', 'Iota',  \n",
    "  'Kappa', 'Lambda', 'Mu', 'S:677H.Robin1', 'S:677P.Pelican']   \n",
    "  \n",
    "total nonUS+other =  19   \n",
    "\n",
    "total unique variants =  24 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aB6-dtiH5OSq",
   "metadata": {
    "id": "aB6-dtiH5OSq"
   },
   "source": [
    "### **`Task.2`** - Find the most processed variant  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bRbxoQb9rbN9",
   "metadata": {
    "id": "bRbxoQb9rbN9"
   },
   "source": [
    "**Tasks**  \n",
    "1. Which variant of COVID-19 has the most sequences processed?  \n",
    "2. Store and print the result in a string called **`variant_most_proc`**  \n",
    "\n",
    "**Useful links**  \n",
    "[pd.DataFrame.groupby](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html#pandas-dataframe-groupby), [pd.DataFrame.aggregate](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.aggregate.html#pandas-dataframe-aggregate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "JoFZ2N-WFDH1",
   "metadata": {
    "id": "JoFZ2N-WFDH1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta\n"
     ]
    }
   ],
   "source": [
    "#=>Enter Your Solution\n",
    "variant_most_proc = df.groupby('variant')['num_sequences'].sum().idxmax()\n",
    "print(variant_most_proc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5uAWiinsdBi2",
   "metadata": {
    "id": "5uAWiinsdBi2"
   },
   "source": [
    "#### **`Task.2 - Expected Outcome`**  \n",
    "```\n",
    "Delta  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663db8ac",
   "metadata": {
    "id": "663db8ac"
   },
   "source": [
    "### **`Task.3`** - Find the best country at processing ALL variant sequences  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ny95wr5_sQhK",
   "metadata": {
    "id": "ny95wr5_sQhK"
   },
   "source": [
    "**`Tasks`**  \n",
    "1. Which country did the best processing **all** categories.    \n",
    "2. Store the result in a string called **`best_proc_country`**  \n",
    "3. The outcome is a single country.  \n",
    "4. **consider** df.groupby(\"location\").aggregate({\"num_sequences\": \"sum\", \"num_sequences_total\": \"sum\"})\n",
    "\n",
    "**`Useful links`**  \n",
    "[youtube: aggregate with groupby and .agg or .aggregate](https://www.youtube.com/watch?v=PNzlx3CjqAE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75c31b41",
   "metadata": {
    "id": "75c31b41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total percent performed by country:\n",
      " location\n",
      "Cyprus                       8.182897\n",
      "Uruguay                      7.973420\n",
      "Hungary                      7.727377\n",
      "Mauritius                    7.317069\n",
      "United Arab Emirates         7.277083\n",
      "                               ...   \n",
      "Sint Maarten (Dutch part)    4.648986\n",
      "Montenegro                   4.597891\n",
      "Moldova                      4.580040\n",
      "Jamaica                      4.563125\n",
      "Kuwait                       4.461042\n",
      "Name: perc_sequences, Length: 121, dtype: float64\n",
      "\n",
      "The best country is  Cyprus\n"
     ]
    }
   ],
   "source": [
    "#=>Enter Your Solution\n",
    "country_perc = df.groupby(\"location\")[\"perc_sequences\"].mean()\n",
    "best_country = country_perc.idxmax()\n",
    "best_proc_country = best_country\n",
    "\n",
    "country_perc_sort = country_perc.sort_values(ascending=False)\n",
    "print(\"Total percent performed by country:\\n\", country_perc_sort)\n",
    "print(\"\\nThe best country is \", best_proc_country)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y0bRJhZLdKBu",
   "metadata": {
    "id": "y0bRJhZLdKBu"
   },
   "source": [
    "#### **`Task.3 - Expected Outcome`**\n",
    "```\n",
    "Total percent performed by country \n",
    "\n",
    "location                percent\n",
    "Cyprus                  8.19\n",
    "Hungary                 7.96\n",
    "Egypt                   7.77\n",
    "United Arab Emirates    7.58\n",
    "Uruguay                 7.26\n",
    "                        ... \n",
    "Seychelles              4.25\n",
    "Fiji                    4.23\n",
    "Slovakia                4.23\n",
    "Brunei                  4.22\n",
    "Vietnam                 4.18\n",
    "Name: perc_sequences, Length: 121, dtype: float64 \n",
    "\n",
    "the best country is =>  Cyprus\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d1d3a0",
   "metadata": {
    "id": "11d1d3a0"
   },
   "source": [
    "### **`Task.4a`** - Find the best country at processing specific variant sequences  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Jxd-0IN00go_",
   "metadata": {
    "id": "Jxd-0IN00go_"
   },
   "source": [
    "**`Tasks`**  \n",
    "1. Which country is best at processing sequences for Alpha, Delta, and Omicron variants?    \n",
    "2. Store and print the result in a string called **`best_proc_country_ADO`** \n",
    "3. The final output is a single country.  \n",
    "\n",
    "**`Useful links`** \n",
    "- ibid  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcafdd2e",
   "metadata": {
    "id": "fcafdd2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location\n",
      "Kuwait        30.655926\n",
      "Moldova       29.894127\n",
      "Montenegro    29.796667\n",
      "Jamaica       29.530370\n",
      "Vietnam       29.365079\n",
      "                ...    \n",
      "Paraguay       5.878803\n",
      "Hungary        4.847647\n",
      "Cyprus         1.203492\n",
      "Madagascar     0.530860\n",
      "Uruguay        0.000000\n",
      "Name: perc_sequences, Length: 121, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#=>Enter Your Solution\n",
    "ado_df = df[df['variant'].isin(['Alpha', 'Delta', 'Omicron'])]\n",
    "country_ado_perc = ado_df.groupby(\"location\")[\"perc_sequences\"].mean()\n",
    "\n",
    "country_ado_perc_sorted = country_ado_perc.sort_values(ascending=False)\n",
    "print(country_ado_perc_sorted)\n",
    "#I've odne this a few differnt ways and just can't seem to get Vietnam, is there a possibility that the data has been changed? It's fifth on this order. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fvANdWH2kOqc",
   "metadata": {
    "id": "fvANdWH2kOqc"
   },
   "source": [
    "#### **`Task.4a - Expected Outcome`**  \n",
    "```\n",
    "best_proc_country_ADO = Vietnam  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccb8782",
   "metadata": {
    "id": "9ccb8782"
   },
   "source": [
    "### **`Task.4b`** - Find the United States ranking for processing Alpha, Delta, and Omicron  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uS-CWHXDFnlB",
   "metadata": {
    "id": "uS-CWHXDFnlB"
   },
   "source": [
    "**`Tasks`**  \n",
    "Given the outcome in 4a\n",
    "1. Find the positional index value for the US ranking for processing sequences for Alpha, Delta, an Omicron variants.   \n",
    "2. Store and print the ranking as an integer in a **us_ranking** variable.  \n",
    "3. Ensure your ranking scale reflects a scale starting at 1.  \n",
    "4. As a refresher, Python indexing starts at 0.  \n",
    "\n",
    "**`Useful links`** \n",
    "- [enumerate](https://docs.python.org/3/library/functions.html#enumerate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "038e81b5",
   "metadata": {
    "id": "038e81b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "#=>Enter Your Solution\n",
    "ado_df = df[df['variant'].isin(['Alpha', 'Delta', 'Omicron'])]\n",
    "\n",
    "country_ado_perc = ado_df.groupby(\"location\")[\"perc_sequences\"].mean()\n",
    "country_ado_perc_sorted = country_ado_perc.sort_values(ascending=False)\n",
    "\n",
    "us_ranking = None\n",
    "for idx, country in enumerate(country_ado_perc_sorted.index):\n",
    "    if country == \"United States\":\n",
    "        us_ranking = idx + 1  #Due to python indexing\n",
    "        break\n",
    "\n",
    "print(us_ranking)\n",
    "#This result is based on my 4a results/list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G5SUmnTckQBM",
   "metadata": {
    "id": "G5SUmnTckQBM"
   },
   "source": [
    "#### **`Task.4b - Expected Outcome`**  \n",
    "```\n",
    "United States ranking = 57  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283e46d3",
   "metadata": {
    "id": "283e46d3"
   },
   "source": [
    "### **`Task.5.<final.task>` - Write instructions for a jr. data scientist assignment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qhqwnKBcvW7R",
   "metadata": {
    "id": "qhqwnKBcvW7R"
   },
   "source": [
    "**`Task =>`**  \n",
    "- Write clear and precise directions that enable your  new junior  \n",
    "- data analyst, aka \"Jr,\" to modify and fix code that you provide.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913dpFR0Fp1E",
   "metadata": {
    "id": "913dpFR0Fp1E"
   },
   "source": [
    "#### **`Grading requirements=>`**  \n",
    "A clear and precise explanation of specific activities for production code your boss needs but you dont have time to fix.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2mN_TFHdT1Vg",
   "metadata": {
    "id": "2mN_TFHdT1Vg"
   },
   "source": [
    "Data science requires clear explanations of tasks, methodology, and effective communication with peers. To help the new junior analyst complete their first assignment, provide a concise and precise description including  \n",
    "\n",
    "1. `Sample Outcome`\n",
    "Deliver a comprehensive report summarizing findings and insights from data analysis. Include, as needed, desired outcome format, data objects, and visualizations.  \n",
    "\n",
    "2. `Python Code Explanation`\n",
    "Use plain language to describe specific Python code to achieve the desired outcome. Refer to pandas, Python, and other library documentation to incorporate particular language.  \n",
    "\n",
    "3. `Consider Deprecated Functions`\n",
    "The provided code is outdated and broken. Encourage problem-solving skills and leverage previous experience with similar tasks. Provide relevant links for reverse engineering.  \n",
    "\n",
    "**`Additional personnel considerations`**\n",
    "4. `Plain Language Explanation`\n",
    "Consider the junior analyst's background in C and provide clear and unambiguous instructions.  \n",
    "\n",
    "5. `Documentation Reference`\n",
    "Emphasize where to consult pandas, Python, and other library documentation to discern code mechanics and clarify concepts.  \n",
    "\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TOdZMTe-Rcl_",
   "metadata": {
    "id": "TOdZMTe-Rcl_"
   },
   "source": [
    "`Your manager's original request => [memo substrate]` \n",
    "\n",
    "`\"hey! i need by lunch` the processed sequences per country on any date`  \n",
    "because as CFO wants to crunch numbers this afternoon - thx Lambda\"\n",
    "\n",
    "1. Determine the percentage of processed sequences for the Alpha, Delta, and Omicron variants in the US.  \n",
    "2. Store the result as a dictionary where keys are variant names and values are percentages.  \n",
    "3. Save in variable = proc_seq_us\n",
    "\n",
    "`=> Other implied items based on same exercise for manager last year`\n",
    "- Determine each country's total processed sequences for Omicron on December 27, 2021 or any other date entered (date updated from 2020).\n",
    "- provide country name and # processed sequences\n",
    "- bidirectional sorting\n",
    "- store outcomes in tuple like mytuple(country_name, processed_sequen, ) \n",
    "- variables totals like `total_omicron_2021`  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bcda66-8371-4e8a-8d48-6332b31ac7a2",
   "metadata": {
    "id": "sMFoLYaXlRGZ"
   },
   "source": [
    "#=> Enter Your Solution; i.e. write memo here\n",
    "Hi Jr.,\n",
    "\n",
    "Below are concise instructions and guidance for fixing and updating the provided legacy Python/pandas code. The primary goal is to accurately produce two outcomes :\n",
    "\n",
    "Percentage of processed sequences in the US for variants Alpha, Delta, and Omicron.\n",
    "\n",
    "Total processed Omicron sequences per country on December 27, 2021 and any dates.\n",
    "\n",
    "Task and Methodology:\n",
    "\n",
    "Your primary tasks are clearly defined above. Begin by closely examining the provided code. Understand its purpose, identify errors, and correct them. Provide a clear, concise summary report that includes insights from your analysis. Present your results neatly, preferably in tabular formats or visualizations such as bar charts.\n",
    "\n",
    "Python Code Explanation:\n",
    "\n",
    "The provided code involves loading data into pandas DataFrames, filtering based on certain conditions, grouping data, and calculating aggregates like sums and percentages. Carefully review pandas documentation for functions like .groupby(), .loc[], and .agg() to understand their usage clearly. Consult the pandas indexing guide here: pandas indexing & selection in library.\n",
    "\n",
    "Consider Deprecated Functions:\n",
    "\n",
    "Note that some provided code snippets may contain outdated or deprecated pandas functions. Use your problem-solving skills from your C programming experience to debug effectively. Common deprecated pandas functions include .ix[]; always use .loc[] or .iloc[] instead. Refer to pandas documentation on deprecation: pandas deprecation link in library.\n",
    "\n",
    "Additional Personnel Considerations:\n",
    "\n",
    "Given your background in C, approach pandas as manipulating structured data similar to arrays or structs. Pandas allows indexing and filtering data with logical conditions clearly and succinctly. Be attentive to Python’s zero-based indexing and careful about data type conversions.\n",
    "\n",
    "Resources: \n",
    "Python Standard Library\n",
    "Check code for comments that provide a starting point or potential check\n",
    "\n",
    "Full Expectations:\n",
    "Determine the percentage of processed sequences for the Alpha, Delta, and Omicron variants in the US.\n",
    "Store the result as a dictionary where keys are variant names and values are percentages.\n",
    "Save in variable = proc_seq_us\n",
    "\n",
    "Determine each country's total processed sequences for Omicron on December 27, 2021 or any other date entered (date updated from 2020).\n",
    "provide country name and # processed sequences\n",
    "bidirectional sorting\n",
    "store outcomes in tuple like mytuple(country_name, processed_sequen, )\n",
    "variables totals like total_omicron_2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uV6K5AkAfDET",
   "metadata": {
    "id": "uV6K5AkAfDET"
   },
   "source": [
    "#### `5a - code you found from last year's excercise `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "qUkhMZD8T40i",
   "metadata": {
    "id": "qUkhMZD8T40i"
   },
   "outputs": [],
   "source": [
    "#=> I of III - broken code last year\n",
    "import pandas as pd\n",
    "\n",
    "#Read the data file\n",
    "url = \"https://github.com/cosc-526/home.page/raw/main/data.M.1.assignment.covid.data.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "### Partially working code\n",
    "# total_omicron_2021 = []\n",
    "# #5.1\n",
    "# df = df.set_index(\"location\") #Check if setting the index here is necessary or beneficial\n",
    "# #5.2\n",
    "# df = df.loc[df6[\"date\"] == \"2021-12-27\"] #Check that filtering uses the correct df variable\n",
    "# #5.3\n",
    "# df = df6.loc[df6[\"variant\"] == \"Omicron\"] #Check that variant column is filtered correctly, review pandas .loc us\n",
    "# #5.3\n",
    "# df = df6[\"num_sequences\"] #Ensure indexing is correct\n",
    "# #5.4\n",
    "#  = list(zip(df.index, df))\n",
    "# #5.5\n",
    "# df7 = pd.DataFrame(sorted(missing, key=lambda x: x[1], reverse=True))\n",
    "# print(df7)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dzXyv7kGZlBP",
   "metadata": {
    "id": "dzXyv7kGZlBP"
   },
   "source": [
    "##### **`5a - Expected Outcome`**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g1qcVGxqavtv",
   "metadata": {
    "id": "g1qcVGxqavtv"
   },
   "source": [
    "```\n",
    "0\t                1\n",
    "0\tUnited Kingdom\t52456\n",
    "1\tUnited States\t  24681\n",
    "2\tDenmark\t        3331\n",
    "3\tGermany\t        1701\n",
    "4\tIsrael\t        1578\n",
    "...\t...\t...\n",
    "59\tVietnam\t      1\n",
    "60\tMoldova\t      0\n",
    "61\tMonaco\t      0\n",
    "62\tNepal\t        0\n",
    "63\tSouth Korea \t0\n",
    "64 rows × 2 columns\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5G3szw10bNTr",
   "metadata": {
    "id": "5G3szw10bNTr"
   },
   "source": [
    "#### `5b - code you found from last year's excercise `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cf6efa9",
   "metadata": {
    "id": "6cf6efa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Alpha': 11.520951617373877, 'Delta': 63.76796208057254, 'Omicron': 1.370817855027461}\n"
     ]
    }
   ],
   "source": [
    "#=> II of III - broken code last year\n",
    "\n",
    "proc_seq_us = {}\n",
    "df2 = df.groupby([\"location\", \"variant\"]).aggregate({\n",
    "    \"num_sequences\": \"sum\",\n",
    "    \"num_sequences_total\": \"sum\",\n",
    "})\n",
    "df2[\"perc_sequences\"] = (df2[\"num_sequences\"] / df2[\"num_sequences_total\"]) * 100\n",
    "df2 = df2.loc[(\"United States\", [\"Alpha\", \"Delta\", \"Omicron\"]), :].loc[\"United States\"]\n",
    "df2 = df2[\"perc_sequences\"]\n",
    "proc_seq_us = df2.to_dict()\n",
    "print(proc_seq_us)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NfYB_LjbZ_zq",
   "metadata": {
    "id": "NfYB_LjbZ_zq"
   },
   "source": [
    "##### **`5b - Expected Outcome`**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_TI3iusnbUbO",
   "metadata": {
    "id": "_TI3iusnbUbO"
   },
   "source": [
    "```\n",
    "{'Alpha': 11.520951617373877, 'Delta': 63.76796208057254, \n",
    "                                                'Omicron': 1.370817855027461}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ySKYi-77bbXc",
   "metadata": {
    "id": "ySKYi-77bbXc"
   },
   "source": [
    "#### `5c - code you found from last year's excercise `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ilcLFMXTW9jG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ilcLFMXTW9jG",
    "outputId": "2b827e05-e959-4580-861d-518facb7c1c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 0      1\n",
      "0   United Kingdom  52456\n",
      "1    United States  24681\n",
      "2          Denmark   3331\n",
      "3          Germany   1701\n",
      "4           Israel   1578\n",
      "..             ...    ...\n",
      "59         Vietnam      1\n",
      "60         Moldova      0\n",
      "61          Monaco      0\n",
      "62           Nepal      0\n",
      "63     South Korea      0\n",
      "\n",
      "[64 rows x 2 columns]\n",
      "[('United Kingdom', 52456), ('United States', 24681), ('Denmark', 3331), ('Germany', 1701), ('Israel', 1578), ('Australia', 1319), ('Switzerland', 514), ('France', 509), ('Italy', 486), ('Belgium', 464), ('Spain', 461), ('Sweden', 434), ('Chile', 260), ('Netherlands', 254), ('Singapore', 249), ('Mexico', 240), ('Turkey', 202), ('India', 174), ('Brazil', 147), ('Botswana', 142), ('Indonesia', 128), ('Japan', 118), ('Portugal', 118), ('Argentina', 80), ('New Zealand', 63), ('South Africa', 61), ('Lithuania', 50), ('Czechia', 49), ('Georgia', 46), ('Russia', 45), ('Colombia', 37), ('Sri Lanka', 37), ('Hong Kong', 35), ('Malta', 34), ('Poland', 28), ('Ecuador', 26), ('Canada', 25), ('Jordan', 22), ('Malawi', 21), ('Cambodia', 18), ('Norway', 17), ('Morocco', 15), ('Senegal', 15), ('Costa Rica', 14), ('Pakistan', 11), ('Nigeria', 10), ('Peru', 10), ('Brunei', 8), ('Slovakia', 8), ('Trinidad and Tobago', 8), ('Maldives', 7), ('Zambia', 7), ('Thailand', 6), ('Malaysia', 5), ('Bangladesh', 4), ('Romania', 3), ('Iran', 1), ('Oman', 1), ('Ukraine', 1), ('Vietnam', 1), ('Moldova', 0), ('Monaco', 0), ('Nepal', 0), ('South Korea', 0)]\n"
     ]
    }
   ],
   "source": [
    "#=> III of III - broken code last year\n",
    "\n",
    "total_omicron_2021 = []\n",
    "#5.1\n",
    "df6 = df.set_index(\"location\")\n",
    "#5.2\n",
    "df6 = df6.loc[df6[\"date\"] == \"2021-12-27\"]\n",
    "#5.3#\n",
    "df6 = df6.loc[df6[\"variant\"] == \"Omicron\"]\n",
    "#5.3\n",
    "df6 = df6[\"num_sequences\"]\n",
    "#5.4\n",
    "total_omicron_2021 = list(zip(df6.index, df6)) #Zip index and values into tuples? Check for missing variables\n",
    "#5.5\n",
    "df7 = pd.DataFrame(sorted(total_omicron_2021, key=lambda x: x[1], reverse=True))\n",
    "print(df7)\n",
    "total_omicron_2021 = sorted(total_omicron_2021, key=lambda x: x[1], reverse=True) # Correct sorting method, define variables?\n",
    "print(total_omicron_2021) #Confirm and validate correctness of the final data structure and its print output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x3c3REubaSEx",
   "metadata": {
    "id": "x3c3REubaSEx"
   },
   "source": [
    "##### **`5c - Expected Outcome`**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2JNdnFwTap_f",
   "metadata": {
    "id": "2JNdnFwTap_f"
   },
   "source": [
    "```\n",
    "                 0      1\n",
    "0   United Kingdom  52456\n",
    "1    United States  24681\n",
    "2          Denmark   3331\n",
    "3          Germany   1701\n",
    "4           Israel   1578\n",
    "..             ...    ...\n",
    "59         Vietnam      1\n",
    "60         Moldova      0\n",
    "61          Monaco      0\n",
    "62           Nepal      0\n",
    "63     South Korea      0\n",
    "[64 rows x 2 columns]\n",
    "\n",
    "#[('United Kingdom', 52456), ('United States', 24681), ('Denmark', 3331),\n",
    " ('Germany', 1701), ('Israel', 1578), ('Australia', 1319), ('Switzerland', 514),\n",
    "  ('France', 509), ('Italy', 486), ('Belgium', 464), ('Spain', 461), \n",
    "  ('Sweden', 434), ('Chile', 260), ('Netherlands', 254), ('Singapore', 249),\n",
    "  ('Mexico', 240), ('Turkey', 202), ('India', 174), ('Brazil', 147),\n",
    "   ('Botswana', 142), ('Indonesia', 128), ('Japan', 118), ('Portugal', 118),\n",
    "    ('Argentina', 80), ('New Zealand', 63), ('South Africa', 61), \n",
    "    ('Lithuania', 50), ('Czechia', 49), ('Georgia', 46), ('Russia', 45), \n",
    "    ('Colombia', 37), ('Sri Lanka', 37), ('Hong Kong', 35), ('Malta', 34),\n",
    "     ('Poland', 28), ('Ecuador', 26), ('Canada', 25), ('Jordan', 22), \n",
    "     ('Malawi', 21), ('Cambodia', 18), ('Norway', 17), ('Morocco', 15), \n",
    "     ('Senegal', 15), ('Costa Rica', 14), ('Pakistan', 11), ('Nigeria', 10),\n",
    "      ('Peru', 10), ('Brunei', 8), ('Slovakia', 8), ('Trinidad and Tobago', 8),\n",
    "       ('Maldives', 7), ('Zambia', 7), ('Thailand', 6), ('Malaysia', 5), \n",
    "       ('Bangladesh', 4), ('Romania', 3), ('Iran', 1), ('Oman', 1),\n",
    "        ('Ukraine', 1), ('Vietnam', 1), ('Moldova', 0), ('Monaco', 0), \n",
    "        ('Nepal', 0), ('South Korea', 0)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kjA00fVqPjEm",
   "metadata": {
    "id": "kjA00fVqPjEm"
   },
   "source": [
    "## `M.1. Ensure Spark is Enabled`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2DzgneXkPhuC",
   "metadata": {
    "id": "2DzgneXkPhuC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The operation couldn’t be completed. Unable to locate a Java Runtime.\n",
      "Please visit http://www.java.com for information on installing Java.\n",
      "\n",
      "/Users/kevhhu/opt/anaconda3/envs/tf/lib/python3.11/site-packages/pyspark/bin/spark-class: line 97: CMD: bad array subscript\n",
      "head: illegal line count -- -1\n"
     ]
    },
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#Create a SparkSession\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOmicron Sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(spark)\n\u001b[1;32m      6\u001b[0m spark\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.11/site-packages/pyspark/sql/session.py:556\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    554\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    555\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 556\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39mgetOrCreate(sparkConf)\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    559\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.11/site-packages/pyspark/core/context.py:523\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 523\u001b[0m         SparkContext(conf\u001b[38;5;241m=\u001b[39mconf \u001b[38;5;129;01mor\u001b[39;00m SparkConf())\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.11/site-packages/pyspark/core/context.py:205\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    203\u001b[0m     )\n\u001b[0;32m--> 205\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    208\u001b[0m         master,\n\u001b[1;32m    209\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    219\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    220\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.11/site-packages/pyspark/core/context.py:444\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 444\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m launch_gateway(conf)\n\u001b[1;32m    445\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.11/site-packages/pyspark/java_gateway.py:111\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[1;32m    112\u001b[0m         errorClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    113\u001b[0m         messageParameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    114\u001b[0m     )\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[1;32m    117\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"Omicron Sequences\").getOrCreate()\n",
    "print(spark)\n",
    "spark.stop()\n",
    "\n",
    "### If this cell fails, you may need another Notebook environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5967943-ff0a-4dfd-86f5-4bc4589c4e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kevhhu/opt/anaconda3/envs/tf/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f7bed31-e405-4b98-8443-09490d6853af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Using cached pyspark-4.0.0-py2.py3-none-any.whl\n",
      "Collecting py4j==0.10.9.9 (from pyspark)\n",
      "  Using cached py4j-0.10.9.9-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Using cached py4j-0.10.9.9-py2.py3-none-any.whl (203 kB)\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.9 pyspark-4.0.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pyspark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b067937e-2505-4882-ac8d-72aba3375729",
   "metadata": {},
   "source": [
    "#Currently working on getting Java set up to run PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6008df4e-7f07-4419-b547-e8c3dd6b6bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "el9sjRdm9gjv",
    "7o91Ig5QKVif",
    "MqZm_iFTPI7f",
    "JfIew_dgmpCQ",
    "kRoOscDyqoCO",
    "wcrKuRsWXUW1",
    "913dpFR0Fp1E",
    "kjA00fVqPjEm"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
